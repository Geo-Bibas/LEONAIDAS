{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__confluent.support.metrics',\n",
       " '_schemas',\n",
       " 'connect-status',\n",
       " 'connect_configs',\n",
       " 'connect_offsets',\n",
       " 'source.public.grades_streaming',\n",
       " 'source.public.raw_grades',\n",
       " 'test'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "bootstrap_servers = ['localhost:29092']\n",
    "consumer = KafkaConsumer( bootstrap_servers=bootstrap_servers)\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSING AND WRITE TO HUDI USING STRUCTURED STREAMING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active stream stats: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Recent progress: []\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = f64b7897-5c67-4433-8be8-6c168c2ecfe4, runId = 6a6f0405-55c4-4b9a-b760-0ad9bd9e2258] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 272, in call\n    raise e\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_26044\\2848879293.py\", line 142, in process_batch\n    raise e\n  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_26044\\2848879293.py\", line 137, in process_batch\n    .save(\"C:/tmp/spark_warehouse/from_kafka\")\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 968, in save\n    self._jwrite.save(path)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n    return f(*a, **kw)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o1228.save.\n: org.apache.hudi.exception.HoodieUpsertException: Failed upsert schema compatibility check\r\n\tat org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:852)\r\n\tat org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:140)\r\n\tat org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:224)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:431)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy32.call(Unknown Source)\r\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\r\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\r\n\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\r\nCaused by: org.apache.hudi.exception.HoodieException: Failed to read schema/check compatibility for base path C:/tmp/spark_warehouse/from_kafka\r\n\tat org.apache.hudi.table.HoodieTable.validateSchema(HoodieTable.java:840)\r\n\tat org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:850)\r\n\t... 75 more\r\nCaused by: org.apache.hudi.exception.SchemaCompatibilityException: Failed schema compatibility check\nwriterSchema: {\"type\":\"record\",\"name\":\"grades_record\",\"namespace\":\"hoodie.grades\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_commit_seqno\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_record_key\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_partition_path\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_file_name\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"id\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"schoolyear\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"semester\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"code\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"description\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"units\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"instructor_id\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"instructor_name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"srcode\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"fullname\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"campus\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"program\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"major\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"yearlevel\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"curriculum\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"class_section\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_final\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_reexam\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"status\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"processing_time\",\"type\":{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}},{\"name\":\"grade_numeric\",\"type\":[\"null\",{\"type\":\"fixed\",\"name\":\"fixed\",\"namespace\":\"hoodie.grades.grades_record.grade_numeric\",\"size\":3,\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}],\"default\":null},{\"name\":\"grade_classification\",\"type\":\"string\"},{\"name\":\"start_year\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"year_sem\",\"type\":\"string\"},{\"name\":\"program_id\",\"type\":[\"null\",\"int\"],\"default\":null}]}\ntableSchema: {\"type\":\"record\",\"name\":\"grades_record\",\"namespace\":\"hoodie.grades\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_commit_seqno\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_record_key\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_partition_path\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_file_name\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"id\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"schoolyear\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"semester\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"code\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"description\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"units\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"instructor_id\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"instructor_name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"srcode\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"fullname\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"campus\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"program\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"major\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"yearlevel\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"curriculum\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"class_section\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_final\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_reexam\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"status\",\"type\":[\"null\",\"string\"],\"default\":null}]}\r\n\tat org.apache.hudi.avro.AvroSchemaUtils.checkSchemaCompatible(AvroSchemaUtils.java:340)\r\n\tat org.apache.hudi.table.HoodieTable.validateSchema(HoodieTable.java:838)\r\n\t... 76 more\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 156\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActive stream stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecent progress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mrecentProgress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Check status every minute\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\streaming.py:105\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout must be a positive integer or float. Got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout)\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: Query [id = f64b7897-5c67-4433-8be8-6c168c2ecfe4, runId = 6a6f0405-55c4-4b9a-b760-0ad9bd9e2258] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 272, in call\n    raise e\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_26044\\2848879293.py\", line 142, in process_batch\n    raise e\n  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_26044\\2848879293.py\", line 137, in process_batch\n    .save(\"C:/tmp/spark_warehouse/from_kafka\")\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 968, in save\n    self._jwrite.save(path)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n    return f(*a, **kw)\n  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o1228.save.\n: org.apache.hudi.exception.HoodieUpsertException: Failed upsert schema compatibility check\r\n\tat org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:852)\r\n\tat org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:140)\r\n\tat org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:224)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:431)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy32.call(Unknown Source)\r\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\r\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\r\n\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\r\nCaused by: org.apache.hudi.exception.HoodieException: Failed to read schema/check compatibility for base path C:/tmp/spark_warehouse/from_kafka\r\n\tat org.apache.hudi.table.HoodieTable.validateSchema(HoodieTable.java:840)\r\n\tat org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:850)\r\n\t... 75 more\r\nCaused by: org.apache.hudi.exception.SchemaCompatibilityException: Failed schema compatibility check\nwriterSchema: {\"type\":\"record\",\"name\":\"grades_record\",\"namespace\":\"hoodie.grades\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_commit_seqno\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_record_key\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_partition_path\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_file_name\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"id\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"schoolyear\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"semester\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"code\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"description\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"units\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"instructor_id\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"instructor_name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"srcode\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"fullname\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"campus\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"program\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"major\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"yearlevel\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"curriculum\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"class_section\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_final\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_reexam\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"status\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"processing_time\",\"type\":{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}},{\"name\":\"grade_numeric\",\"type\":[\"null\",{\"type\":\"fixed\",\"name\":\"fixed\",\"namespace\":\"hoodie.grades.grades_record.grade_numeric\",\"size\":3,\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}],\"default\":null},{\"name\":\"grade_classification\",\"type\":\"string\"},{\"name\":\"start_year\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"year_sem\",\"type\":\"string\"},{\"name\":\"program_id\",\"type\":[\"null\",\"int\"],\"default\":null}]}\ntableSchema: {\"type\":\"record\",\"name\":\"grades_record\",\"namespace\":\"hoodie.grades\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_commit_seqno\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_record_key\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_partition_path\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"_hoodie_file_name\",\"type\":[\"null\",\"string\"],\"doc\":\"\",\"default\":null},{\"name\":\"id\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"schoolyear\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"semester\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"code\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"description\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"units\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"instructor_id\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"instructor_name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"srcode\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"fullname\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"campus\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"program\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"major\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"yearlevel\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"curriculum\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"class_section\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_final\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"grade_reexam\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"status\",\"type\":[\"null\",\"string\"],\"default\":null}]}\r\n\tat org.apache.hudi.avro.AvroSchemaUtils.checkSchemaCompatible(AvroSchemaUtils.java:340)\r\n\tat org.apache.hudi.table.HoodieTable.validateSchema(HoodieTable.java:838)\r\n\t... 76 more\r\n\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType\n",
    ")\n",
    "import sys\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with additional Kafka configs\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config(\"spark.jars.packages\", \n",
    "                (\"org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,\"\n",
    "                 \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "                 \"org.apache.kafka:kafka-clients:3.3.0\"))\n",
    "         .config('spark.sql.extensions', 'org.apache.hudi.spark3.sql.HoodieSparkSessionExtension')\n",
    "         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog')\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Define Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",  # Update with your Kafka broker\n",
    "    \"subscribe\": \"source.public.grades_streaming\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"maxOffsetsPerTrigger\": \"100\"  # Control batch size\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\")\n",
    "             .withColumn(\"processing_time\", current_timestamp()))\n",
    "\n",
    "# Enhanced Hudi options\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.SimpleKeyGenerator'\n",
    "}\n",
    "\n",
    "# Enhanced batch processing function\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.count() > 0:\n",
    "            # Apply transformations\n",
    "            transformed_df = transform(batch_df, spark)\n",
    "            \n",
    "            # Write to Hudi with error handling\n",
    "            transformed_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            print(f\"Successfully processed batch {batch_id} with {batch_df.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Start streaming with improved configuration\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"2 minutes\")\n",
    "         .start())\n",
    "\n",
    "# Add query monitoring\n",
    "while query.isActive:\n",
    "    print(f\"Active stream stats: {query.status}\")\n",
    "    print(f\"Recent progress: {query.recentProgress}\")\n",
    "    query.awaitTermination(60)  # Check status every minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS AND WRITE TO BOTH HUDI AND DATABASE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@Geo_Lappy.mshome.net:53849\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:244)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:585)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 89\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Configure Spark with Kafka and Hudi dependencies\u001b[39;00m\n\u001b[0;32m     76\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKafkaToHudiProcessor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.serializer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.serializer.KryoSerializer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match your Spark version\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.postgresql:postgresql:42.7.5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# PostgreSQL driver\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.javax.jdo.option.ConnectionDriverName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hudi.spark3.sql.HoodieSparkSessionExtension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.hudi.catalog.HoodieCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.streaming.stopGracefullyOnShutdown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.streaming.checkpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/tmp/spark_warehouse/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 89\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Optimized Kafka configuration\u001b[39;00m\n\u001b[0;32m     92\u001b[0m kafka_options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost:29092\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource.public.grades_streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxOffsetsPerTrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Adjust based on your needs\u001b[39;00m\n\u001b[0;32m    101\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@Geo_Lappy.mshome.net:53849\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:244)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:585)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType\n",
    ")\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "\n",
    "    # Add processing_time column\n",
    "    #df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with Kafka and Hudi dependencies\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config('spark.jars.packages', \n",
    "                ('org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,'  # Match your Spark version\n",
    "                 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,'\n",
    "                 'org.postgresql:postgresql:42.7.5'))  # PostgreSQL driver\n",
    "         .config('spark.hadoop.javax.jdo.option.ConnectionDriverName', 'org.postgresql.Driver')\n",
    "         .config(\"spark.sql.extensions\", \"org.apache.hudi.spark3.sql.HoodieSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Optimized Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",\n",
    "    \"subscribe\": \"source.public.grades_streaming\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"fetchOffset.numRetries\": \"5\",\n",
    "    \"fetch.max.wait.ms\": \"5000\",\n",
    "    \"maxOffsetsPerTrigger\": \"100\"  # Adjust based on your needs\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\"))\n",
    "\n",
    "# Enhanced Hudi options for update handling\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',  # Use upsert for updates\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',  # Change back to processing_time\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.write.concurrency.mode': 'optimistic_concurrency_control',\n",
    "    'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.InProcessLockProvider'\n",
    "}\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.rdd.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "            return\n",
    "\n",
    "        # Cache the batch for multiple operations\n",
    "        batch_df.cache()\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_df = transform(batch_df, spark)\n",
    "        transformed_df.cache()\n",
    "        \n",
    "        try:\n",
    "            # Add processing_time for Hudi\n",
    "            hudi_df = transformed_df.withColumn(\"processing_time\", current_timestamp())\n",
    "            \n",
    "            # Write to Hudi\n",
    "            hudi_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            transformed_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://localhost:5433/CAIST\") \\\n",
    "            .option(\"dbtable\", \"processed_grades\") \\\n",
    "            .option(\"user\", \"postgres\") \\\n",
    "            .option(\"password\", \"password\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "            # Log metrics\n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            input_count = batch_df.count()\n",
    "            output_count = transformed_df.count()\n",
    "            \n",
    "            print(f\"\"\"\n",
    "            Batch {batch_id} metrics:\n",
    "            - Processing time: {duration:.2f} seconds\n",
    "            - Input records: {input_count}\n",
    "            - Output records: {output_count}\n",
    "            - Processing rate: {output_count/duration:.2f} records/second\n",
    "            \"\"\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up cached DataFrames\n",
    "            transformed_df.unpersist()\n",
    "            hudi_df.unpersist()\n",
    "            batch_df.unpersist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Start streaming with monitoring\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"2 minutes\")\n",
    "         .start())\n",
    "\n",
    "# Enhanced monitoring\n",
    "while query.isActive:\n",
    "    try:\n",
    "        stats = query.status\n",
    "        progress = query.recentProgress\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Stream Status: {stats['message']}\n",
    "        Data Available: {stats.get('isDataAvailable', False)}\n",
    "        Trigger Active: {stats.get('isTriggerActive', False)}\n",
    "        Records Processed: {progress[-1]['numInputRows'] if progress else 0}\n",
    "        \"\"\")\n",
    "        \n",
    "        query.awaitTermination(30)  # Check every 2 minutes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING UPSERTS TO DB # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Stream Status: Initializing sources\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "Error processing batch 26: An error occurred while calling o594.save.\n",
      ": org.postgresql.util.PSQLException: ERROR: syntax error at or near \"(\"\n",
      "  Position: 14\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:356)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:341)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:317)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1060)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:890)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy32.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "Monitoring error: Query [id = 6cab5663-134c-4ab0-9ff8-9c5f52cdafff, runId = 4096204f-3eb8-4a65-9d99-dcf033772764] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_21552\\1456830187.py\", line 245, in process_batch\n",
      "    raise e\n",
      "  File \"C:\\Users\\denve\\AppData\\Local\\Temp\\ipykernel_21552\\1456830187.py\", line 221, in process_batch\n",
      "    .save()\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 966, in save\n",
      "    self._jwrite.save()\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"c:\\Users\\denve\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o594.save.\n",
      ": org.postgresql.util.PSQLException: ERROR: syntax error at or near \"(\"\n",
      "  Position: 14\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:356)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:341)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:317)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1060)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:890)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy32.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType\n",
    ")\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "\n",
    "    # Add processing_time column\n",
    "    #df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with Kafka and Hudi dependencies\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config('spark.jars.packages', \n",
    "                ('org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,'  # Match your Spark version\n",
    "                 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,'\n",
    "                 'org.postgresql:postgresql:42.7.5'))  # PostgreSQL driver\n",
    "         .config('spark.hadoop.javax.jdo.option.ConnectionDriverName', 'org.postgresql.Driver')\n",
    "         .config(\"spark.sql.extensions\", \"org.apache.hudi.spark3.sql.HoodieSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Optimized Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",\n",
    "    \"subscribe\": \"source.public.grades_streaming\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"fetchOffset.numRetries\": \"5\",\n",
    "    \"fetch.max.wait.ms\": \"5000\",\n",
    "    \"maxOffsetsPerTrigger\": \"100\"  # Adjust based on your needs\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\"))\n",
    "\n",
    "# Enhanced Hudi options for update handling\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',  # Use upsert for updates\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',  # Change back to processing_time\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.write.concurrency.mode': 'optimistic_concurrency_control',\n",
    "    'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.InProcessLockProvider'\n",
    "}\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.rdd.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "            return\n",
    "\n",
    "        # Cache the batch for multiple operations\n",
    "        batch_df.cache()\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_df = transform(batch_df, spark)\n",
    "        transformed_df.cache()\n",
    "        \n",
    "        try:\n",
    "            # Add processing_time for Hudi\n",
    "            hudi_df = transformed_df.withColumn(\"processing_time\", current_timestamp())\n",
    "            \n",
    "            # Write to Hudi\n",
    "            hudi_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            # Inside process_batch function, modify the PostgreSQL write section:\n",
    "            transformed_df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:postgresql://localhost:5433/CAIST\") \\\n",
    "                .option(\"dbtable\", \"\"\"(\n",
    "                    CREATE TABLE IF NOT EXISTS processed_grades (\n",
    "                        id BIGINT PRIMARY KEY,\n",
    "                        schoolyear VARCHAR,\n",
    "                        semester VARCHAR,\n",
    "                        code VARCHAR,\n",
    "                        description VARCHAR, \n",
    "                        units INTEGER,\n",
    "                        instructor_id VARCHAR,\n",
    "                        instructor_name VARCHAR,\n",
    "                        srcode VARCHAR,\n",
    "                        fullname VARCHAR,\n",
    "                        campus VARCHAR,\n",
    "                        program VARCHAR,\n",
    "                        major VARCHAR,\n",
    "                        yearlevel VARCHAR,\n",
    "                        curriculum VARCHAR,\n",
    "                        class_section VARCHAR,\n",
    "                        grade_final VARCHAR,\n",
    "                        grade_reexam VARCHAR,\n",
    "                        status VARCHAR,\n",
    "                        grade_numeric DECIMAL(5,2),\n",
    "                        grade_classification VARCHAR,\n",
    "                        year_sem VARCHAR,\n",
    "                        program_id INTEGER\n",
    "                    );\n",
    "                    INSERT INTO processed_grades VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT (id) DO UPDATE SET\n",
    "                        schoolyear = EXCLUDED.schoolyear,\n",
    "                        semester = EXCLUDED.semester,\n",
    "                        code = EXCLUDED.code,\n",
    "                        description = EXCLUDED.description,\n",
    "                        units = EXCLUDED.units,\n",
    "                        instructor_id = EXCLUDED.instructor_id,\n",
    "                        instructor_name = EXCLUDED.instructor_name,\n",
    "                        srcode = EXCLUDED.srcode,\n",
    "                        fullname = EXCLUDED.fullname,\n",
    "                        campus = EXCLUDED.campus,\n",
    "                        program = EXCLUDED.program,\n",
    "                        major = EXCLUDED.major,\n",
    "                        yearlevel = EXCLUDED.yearlevel,\n",
    "                        curriculum = EXCLUDED.curriculum,\n",
    "                        class_section = EXCLUDED.class_section,\n",
    "                        grade_final = EXCLUDED.grade_final,\n",
    "                        grade_reexam = EXCLUDED.grade_reexam,\n",
    "                        status = EXCLUDED.status,\n",
    "                        grade_numeric = EXCLUDED.grade_numeric,\n",
    "                        grade_classification = EXCLUDED.grade_classification,\n",
    "                        year_sem = EXCLUDED.year_sem,\n",
    "                        program_id = EXCLUDED.program_id\n",
    "                )\"\"\") \\\n",
    "                .option(\"user\", \"postgres\") \\\n",
    "                .option(\"password\", \"password\") \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"batchsize\", \"1000\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "\n",
    "            # Log metrics\n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            input_count = batch_df.count()\n",
    "            output_count = transformed_df.count()\n",
    "            \n",
    "            print(f\"\"\"\n",
    "            Batch {batch_id} metrics:\n",
    "            - Processing time: {duration:.2f} seconds\n",
    "            - Input records: {input_count}\n",
    "            - Output records: {output_count}\n",
    "            - Processing rate: {output_count/duration:.2f} records/second\n",
    "            \"\"\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up cached DataFrames\n",
    "            transformed_df.unpersist()\n",
    "            hudi_df.unpersist()\n",
    "            batch_df.unpersist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Start streaming with monitoring\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"2 minutes\")\n",
    "         .start())\n",
    "\n",
    "# Enhanced monitoring\n",
    "while query.isActive:\n",
    "    try:\n",
    "        stats = query.status\n",
    "        progress = query.recentProgress\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Stream Status: {stats['message']}\n",
    "        Data Available: {stats.get('isDataAvailable', False)}\n",
    "        Trigger Active: {stats.get('isTriggerActive', False)}\n",
    "        Records Processed: {progress[-1]['numInputRows'] if progress else 0}\n",
    "        \"\"\")\n",
    "        \n",
    "        query.awaitTermination(30)  # Check every 2 minutes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Psycopg 2 ETL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Stream Status: Initializing sources\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250402102203797|20250402102203797...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-02 10:22:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "Inserted/Updated 99 records into PostgreSQL.\n",
      "Batch 17 processed in 28.08 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100105762|20250403100105762...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:01:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 18 processed in 14.90 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100105762|20250403100105762...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:01:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 19 processed in 15.49 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100105762|20250403100105762...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:01:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Error writing to PostgreSQL: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "Batch 20 processed in 13.70 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100207376|20250403100207376...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:02:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 21 processed in 12.28 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100207376|20250403100207376...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:02:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 22 processed in 12.69 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100207376|20250403100207376...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:02:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 23 processed in 14.54 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100331930|20250403100331930...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:03:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 24 processed in 16.21 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100331930|20250403100331930...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:03:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 25 processed in 15.61 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100432088|20250403100432088...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:04:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 26 processed in 15.47 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100432088|20250403100432088...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:04:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 27 processed in 14.91 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100432088|20250403100432088...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:04:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 28 processed in 14.48 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100432088|20250403100432088...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:04:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 29 processed in 14.88 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100432088|20250403100432088...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:04:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 30 processed in 15.05 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100701938|20250403100701938...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:07:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 31 processed in 12.67 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100701938|20250403100701938...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:07:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 32 processed in 14.40 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100701938|20250403100701938...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:07:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Error writing to PostgreSQL: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "Batch 33 processed in 15.22 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 30 records into PostgreSQL.\n",
      "Batch 34 processed in 11.30 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 62\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 1 records into PostgreSQL.\n",
      "Batch 35 processed in 9.76 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 2\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "No data to write.\n",
      "Batch 36 processed in 8.94 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 2\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "No data to write.\n",
      "Batch 37 processed in 8.26 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 2\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 1 records into PostgreSQL.\n",
      "Batch 38 processed in 7.85 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 2\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|    id|semester|   code|         description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             program|major|yearlevel|curriculum|class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|        year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "|  20250403100831805|20250403100831805...|            122477|             2023-2024|ca793989-2663-40e...|122477|  SUMMER|GEd 103|Life and Works of...|    3|       -13692|LN-13692, FN-13692| 23255|LN23255, FN23255|ALANGILAN|BS Mechatronics E...| null|    FIRST| 2023-2024|    MEXE-1301|       1.50|        null|PASSED|         1.50|              NORMAL|      2023|2023-2024-SUMMER|        59|2025-04-03 10:08:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+-------+--------------------+-----+-------------+------------------+------+----------------+---------+--------------------+-----+---------+----------+-------------+-----------+------------+------+-------------+--------------------+----------+----------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "Inserted/Updated 1 records into PostgreSQL.\n",
      "Batch 39 processed in 8.79 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 2\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.grades_streaming]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import psycopg2, os\n",
    "from psycopg2.extras import execute_values\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (    \n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType\n",
    ")\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "\n",
    "    # Add processing_time column\n",
    "    #df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with Kafka and Hudi dependencies\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config('spark.jars.packages', \n",
    "                ('org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,'  # Match your Spark version\n",
    "                 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,'\n",
    "                 'org.postgresql:postgresql:42.7.5'))  # PostgreSQL driver\n",
    "         .config('spark.hadoop.javax.jdo.option.ConnectionDriverName', 'org.postgresql.Driver')\n",
    "         .config(\"spark.sql.extensions\", \"org.apache.hudi.spark3.sql.HoodieSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Optimized Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",\n",
    "    \"subscribe\": \"source.public.grades_streaming\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"fetchOffset.numRetries\": \"5\",\n",
    "    \"fetch.max.wait.ms\": \"5000\",\n",
    "    #\"maxOffsetsPerTrigger\": \"100\"  # Adjust based on your needs\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\"))\n",
    "\n",
    "# Enhanced Hudi options for update handling\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',  # Use upsert for updates\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',  # Change back to processing_time\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.write.concurrency.mode': 'optimistic_concurrency_control',\n",
    "    'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.InProcessLockProvider'\n",
    "}\n",
    "\n",
    "def initialize_hudi_table():\n",
    "    table_path = \"C:/tmp/spark_warehouse/from_kafka\"\n",
    "    \n",
    "    if not os.path.exists(table_path) or not os.listdir(table_path):\n",
    "        print(\"Hudi table does not exist. Initializing now...\")\n",
    "\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"hudi\") \\\n",
    "            .options(**hudi_options) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(table_path)\n",
    "        \n",
    "        print(\"Hudi table initialized successfully.\")\n",
    "    else:\n",
    "        try:\n",
    "            spark.read.format(\"hudi\").load(table_path).show(1)\n",
    "            print(\"Hudi table exists. Proceeding...\")\n",
    "        except AnalysisException:\n",
    "            print(\"Error reading Hudi table. Reinitializing...\")\n",
    "            empty_df = spark.createDataFrame([], schema)\n",
    "            empty_df.write.format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save(table_path)\n",
    "        \n",
    "        print(\"Hudi table initialized successfully.\")\n",
    "\n",
    "def write_to_postgres(transformed_df):\n",
    "    \"\"\"\n",
    "    Writes the transformed Spark DataFrame to PostgreSQL using Psycopg2.\n",
    "    Uses batch insert/update for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    db_params = {\n",
    "        \"dbname\": \"CAIST\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"password\",\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": \"5433\"\n",
    "    }\n",
    "    \n",
    "    # Collect data from Spark DataFrame into a list of tuples\n",
    "    data = [tuple(row) for row in transformed_df.collect()]\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to write.\")\n",
    "        return\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO processed_grades (\n",
    "            id, schoolyear, semester, code, description, units, instructor_id, \n",
    "            instructor_name, srcode, fullname, campus, program, major, yearlevel, \n",
    "            curriculum, class_section, grade_final, grade_reexam, status, \n",
    "            grade_numeric, grade_classification, start_year, year_sem, program_id\n",
    "        ) VALUES %s \n",
    "        ON CONFLICT (id) DO UPDATE SET \n",
    "            schoolyear = EXCLUDED.schoolyear,\n",
    "            semester = EXCLUDED.semester,\n",
    "            code = EXCLUDED.code,\n",
    "            description = EXCLUDED.description,\n",
    "            units = EXCLUDED.units,\n",
    "            instructor_id = EXCLUDED.instructor_id,\n",
    "            instructor_name = EXCLUDED.instructor_name,\n",
    "            srcode = EXCLUDED.srcode,\n",
    "            fullname = EXCLUDED.fullname,\n",
    "            campus = EXCLUDED.campus,\n",
    "            program = EXCLUDED.program,\n",
    "            major = EXCLUDED.major,\n",
    "            yearlevel = EXCLUDED.yearlevel,\n",
    "            curriculum = EXCLUDED.curriculum,\n",
    "            class_section = EXCLUDED.class_section,\n",
    "            grade_final = EXCLUDED.grade_final,\n",
    "            grade_reexam = EXCLUDED.grade_reexam,\n",
    "            status = EXCLUDED.status,\n",
    "            grade_numeric = EXCLUDED.grade_numeric,\n",
    "            grade_classification = EXCLUDED.grade_classification,\n",
    "            start_year = EXCLUDED.start_year,\n",
    "            year_sem = EXCLUDED.year_sem,\n",
    "            program_id = EXCLUDED.program_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(**db_params) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                execute_values(cur, insert_query, data)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted/Updated {len(data)} records into PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to PostgreSQL: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.rdd.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "            return\n",
    "        \n",
    "        batch_df.cache()\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Ensure the Hudi table is initialized before writing\n",
    "        initialize_hudi_table()\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_df = transform(batch_df, spark)\n",
    "        transformed_df.cache()\n",
    "        \n",
    "        try:\n",
    "            # Write to Hudi\n",
    "            hudi_df = transformed_df.withColumn(\"processing_time\", current_timestamp())\n",
    "            hudi_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            # Write to PostgreSQL using Psycopg2\n",
    "            write_to_postgres(transformed_df)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            print(f\"Batch {batch_id} processed in {duration:.2f} seconds\")\n",
    "        \n",
    "        finally:\n",
    "            transformed_df.unpersist()\n",
    "            hudi_df.unpersist()\n",
    "            batch_df.unpersist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Start streaming with monitoring\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"30 seconds\")\n",
    "         .start())\n",
    "\n",
    "# Enhanced monitoring\n",
    "while query.isActive:\n",
    "    try:\n",
    "        stats = query.status\n",
    "        progress = query.recentProgress\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Stream Status: {stats['message']}\n",
    "        Data Available: {stats.get('isDataAvailable', False)}\n",
    "        Trigger Active: {stats.get('isTriggerActive', False)}\n",
    "        Records Processed: {progress[-1]['numInputRows'] if progress else 0}\n",
    "        \"\"\")\n",
    "        \n",
    "        query.awaitTermination(30)  # Check every 2 minutes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING ETL EXCEPT INITIALIZING HUDI, TABLE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Stream Status: Initializing sources\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "Error writing to PostgreSQL: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "Batch 0 processed in 14.66 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Error writing to PostgreSQL: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "Batch 1 processed in 7.58 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 2 processed in 9.56 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 3 processed in 12.13 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 4 processed in 12.60 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 82 records into PostgreSQL.\n",
      "Batch 5 processed in 10.71 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 6 processed in 12.98 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "Inserted/Updated 100 records into PostgreSQL.\n",
      "Batch 7 processed in 11.92 seconds\n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n",
      "\n",
      "        Stream Status: Waiting for next trigger\n",
      "        Data Available: True\n",
      "        Trigger Active: False\n",
      "        Records Processed: 200\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType\n",
    ")\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "\n",
    "    # Add processing_time column\n",
    "    #df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with Kafka and Hudi dependencies\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config('spark.jars.packages', \n",
    "                ('org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,'  # Match your Spark version\n",
    "                 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,'\n",
    "                 'org.postgresql:postgresql:42.7.5'))  # PostgreSQL driver\n",
    "         .config('spark.hadoop.javax.jdo.option.ConnectionDriverName', 'org.postgresql.Driver')\n",
    "         .config(\"spark.sql.extensions\", \"org.apache.hudi.spark3.sql.HoodieSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Optimized Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",\n",
    "    \"subscribe\": \"source.public.grades_streaming\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"fetchOffset.numRetries\": \"5\",\n",
    "    \"fetch.max.wait.ms\": \"5000\",\n",
    "    \"maxOffsetsPerTrigger\": \"100\"  # Adjust based on your needs\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\"))\n",
    "\n",
    "# Enhanced Hudi options for update handling\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',  # Use upsert for updates\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',  # Change back to processing_time\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.write.concurrency.mode': 'optimistic_concurrency_control',\n",
    "    'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.InProcessLockProvider'\n",
    "}\n",
    "\n",
    "def write_to_postgres(transformed_df):\n",
    "    \"\"\"\n",
    "    Writes the transformed Spark DataFrame to PostgreSQL using Psycopg2.\n",
    "    Uses batch insert/update for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    db_params = {\n",
    "        \"dbname\": \"caist_db_v4\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"postgres\",\n",
    "        \"host\": \"192.168.20.11\",\n",
    "        \"port\": \"5432\"\n",
    "    }\n",
    "    \n",
    "    # Collect data from Spark DataFrame into a list of tuples\n",
    "    data = [tuple(row) for row in transformed_df.collect()]\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to write.\")\n",
    "        return\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO processed_grades (\n",
    "            id, schoolyear, semester, code, description, units, instructor_id, \n",
    "            instructor_name, srcode, fullname, campus, program, major, yearlevel, \n",
    "            curriculum, class_section, grade_final, grade_reexam, status, \n",
    "            grade_numeric, grade_classification, start_year, year_sem, program_id\n",
    "        ) VALUES %s \n",
    "        ON CONFLICT (id) DO UPDATE SET \n",
    "            schoolyear = EXCLUDED.schoolyear,\n",
    "            semester = EXCLUDED.semester,\n",
    "            code = EXCLUDED.code,\n",
    "            description = EXCLUDED.description,\n",
    "            units = EXCLUDED.units,\n",
    "            instructor_id = EXCLUDED.instructor_id,\n",
    "            instructor_name = EXCLUDED.instructor_name,\n",
    "            srcode = EXCLUDED.srcode,\n",
    "            fullname = EXCLUDED.fullname,\n",
    "            campus = EXCLUDED.campus,\n",
    "            program = EXCLUDED.program,\n",
    "            major = EXCLUDED.major,\n",
    "            yearlevel = EXCLUDED.yearlevel,\n",
    "            curriculum = EXCLUDED.curriculum,\n",
    "            class_section = EXCLUDED.class_section,\n",
    "            grade_final = EXCLUDED.grade_final,\n",
    "            grade_reexam = EXCLUDED.grade_reexam,\n",
    "            status = EXCLUDED.status,\n",
    "            grade_numeric = EXCLUDED.grade_numeric,\n",
    "            grade_classification = EXCLUDED.grade_classification,\n",
    "            start_year = EXCLUDED.start_year,\n",
    "            year_sem = EXCLUDED.year_sem,\n",
    "            program_id = EXCLUDED.program_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(**db_params) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                execute_values(cur, insert_query, data)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted/Updated {len(data)} records into PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to PostgreSQL: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.rdd.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "            return\n",
    "        \n",
    "        batch_df.cache()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_df = transform(batch_df, spark)\n",
    "        transformed_df.cache()\n",
    "        \n",
    "        try:\n",
    "            # Write to Hudi\n",
    "            hudi_df = transformed_df.withColumn(\"processing_time\", current_timestamp())\n",
    "            hudi_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            # Write to PostgreSQL using Psycopg2\n",
    "            write_to_postgres(transformed_df)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            print(f\"Batch {batch_id} processed in {duration:.2f} seconds\")\n",
    "        \n",
    "        finally:\n",
    "            transformed_df.unpersist()\n",
    "            hudi_df.unpersist()\n",
    "            batch_df.unpersist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Start streaming with monitoring\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"2 minutes\")\n",
    "         .start())\n",
    "\n",
    "# Enhanced monitoring\n",
    "while query.isActive:\n",
    "    try:\n",
    "        stats = query.status\n",
    "        progress = query.recentProgress\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Stream Status: {stats['message']}\n",
    "        Data Available: {stats.get('isDataAvailable', False)}\n",
    "        Trigger Active: {stats.get('isTriggerActive', False)}\n",
    "        Records Processed: {progress[-1]['numInputRows'] if progress else 0}\n",
    "        \"\"\")\n",
    "        \n",
    "        query.awaitTermination(30)  # Check every 2 minutes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Kafka consumer setup\n",
    "topicName = 'source.public.raw_grades'\n",
    "consumer = KafkaConsumer(topicName, auto_offset_reset='latest', \n",
    "                         bootstrap_servers=bootstrap_servers, group_id='grades-group')\n",
    "\n",
    "# Process each message and write to Hudi\n",
    "for msg in consumer:\n",
    "    # Print the message for monitoring\n",
    "    message_data = json.loads(msg.value)\n",
    "    print(\"Received message:\", message_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAN CLEANING THROUGH ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Stream Status: Initializing sources\n",
      "        Data Available: False\n",
      "        Trigger Active: False\n",
      "        Records Processed: 0\n",
      "        \n",
      "Hudi table does not exist. Initializing now...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "Inserted/Updated 49594 records into PostgreSQL.\n",
      "Batch 0 processed in 168.17 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50376\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50376\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50376\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50376\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50376\n",
      "        \n",
      "Inserted/Updated 49602 records into PostgreSQL.\n",
      "Batch 1 processed in 135.14 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50885\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50885\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50885\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50885\n",
      "        \n",
      "Inserted/Updated 49665 records into PostgreSQL.\n",
      "Batch 2 processed in 137.35 seconds\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50794\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50794\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50794\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50794\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50794\n",
      "        \n",
      "Inserted/Updated 49692 records into PostgreSQL.\n",
      "Batch 3 processed in 136.45 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50710\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50710\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50710\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50710\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50710\n",
      "        \n",
      "Inserted/Updated 49656 records into PostgreSQL.\n",
      "Batch 4 processed in 132.30 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50901\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50901\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50901\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50901\n",
      "        \n",
      "Inserted/Updated 49637 records into PostgreSQL.\n",
      "Batch 5 processed in 131.94 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50592\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50592\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50592\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50592\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50592\n",
      "        \n",
      "Inserted/Updated 49600 records into PostgreSQL.\n",
      "Batch 6 processed in 136.77 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50768\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50768\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50768\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50768\n",
      "        \n",
      "Inserted/Updated 49645 records into PostgreSQL.\n",
      "Batch 7 processed in 143.53 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50605\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50605\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50605\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50605\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50605\n",
      "        \n",
      "Inserted/Updated 49677 records into PostgreSQL.\n",
      "Batch 8 processed in 141.86 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50777\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50777\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50777\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50777\n",
      "        \n",
      "Inserted/Updated 49698 records into PostgreSQL.\n",
      "Batch 9 processed in 129.73 seconds\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50790\n",
      "        \n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50790\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50790\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50790\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50790\n",
      "        \n",
      "Inserted/Updated 49557 records into PostgreSQL.\n",
      "Batch 10 processed in 129.50 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51007\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51007\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51007\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51007\n",
      "        \n",
      "Inserted/Updated 49696 records into PostgreSQL.\n",
      "Batch 11 processed in 130.63 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50617\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50617\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50617\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50617\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50617\n",
      "        \n",
      "Inserted/Updated 49608 records into PostgreSQL.\n",
      "Batch 12 processed in 132.71 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "Inserted/Updated 49616 records into PostgreSQL.\n",
      "Batch 13 processed in 137.23 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50627\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50627\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50627\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50627\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50627\n",
      "        \n",
      "Inserted/Updated 49613 records into PostgreSQL.\n",
      "Batch 14 processed in 138.14 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50949\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50949\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50949\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50949\n",
      "        \n",
      "Inserted/Updated 49564 records into PostgreSQL.\n",
      "Batch 15 processed in 130.54 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50795\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50795\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50795\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50795\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50795\n",
      "        \n",
      "Inserted/Updated 49626 records into PostgreSQL.\n",
      "Batch 16 processed in 133.64 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50871\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50871\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50871\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50871\n",
      "        \n",
      "Inserted/Updated 49654 records into PostgreSQL.\n",
      "Batch 17 processed in 133.35 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50658\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50658\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50658\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50658\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50658\n",
      "        \n",
      "Inserted/Updated 49645 records into PostgreSQL.\n",
      "Batch 18 processed in 134.60 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "Inserted/Updated 49499 records into PostgreSQL.\n",
      "Batch 19 processed in 151.23 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50668\n",
      "        \n",
      "Inserted/Updated 49596 records into PostgreSQL.\n",
      "Batch 20 processed in 135.82 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50750\n",
      "        \n",
      "Inserted/Updated 49633 records into PostgreSQL.\n",
      "Batch 21 processed in 141.73 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50656\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50656\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50656\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50656\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50656\n",
      "        \n",
      "Inserted/Updated 49500 records into PostgreSQL.\n",
      "Batch 22 processed in 140.43 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50997\n",
      "        \n",
      "Inserted/Updated 49648 records into PostgreSQL.\n",
      "Batch 23 processed in 183.25 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50896\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50896\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50896\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50896\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50896\n",
      "        \n",
      "Inserted/Updated 49592 records into PostgreSQL.\n",
      "Batch 24 processed in 159.65 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50812\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50812\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50812\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50812\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50812\n",
      "        \n",
      "Inserted/Updated 49642 records into PostgreSQL.\n",
      "Batch 25 processed in 151.77 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50887\n",
      "        \n",
      "Inserted/Updated 49553 records into PostgreSQL.\n",
      "Batch 26 processed in 154.06 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "Inserted/Updated 49709 records into PostgreSQL.\n",
      "Batch 27 processed in 157.77 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "Inserted/Updated 49595 records into PostgreSQL.\n",
      "Batch 28 processed in 169.55 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "Inserted/Updated 49637 records into PostgreSQL.\n",
      "Batch 29 processed in 167.45 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50518\n",
      "        \n",
      "Inserted/Updated 49565 records into PostgreSQL.\n",
      "Batch 30 processed in 179.56 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "Inserted/Updated 49604 records into PostgreSQL.\n",
      "Batch 31 processed in 170.51 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50705\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50705\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50705\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50705\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50705\n",
      "        \n",
      "Inserted/Updated 49662 records into PostgreSQL.\n",
      "Batch 32 processed in 158.46 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50659\n",
      "        \n",
      "Inserted/Updated 49624 records into PostgreSQL.\n",
      "Batch 33 processed in 185.10 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "Inserted/Updated 49688 records into PostgreSQL.\n",
      "Batch 34 processed in 179.39 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50900\n",
      "        \n",
      "Inserted/Updated 49676 records into PostgreSQL.\n",
      "Batch 35 processed in 175.97 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50877\n",
      "        \n",
      "Inserted/Updated 49664 records into PostgreSQL.\n",
      "Batch 36 processed in 167.65 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50942\n",
      "        \n",
      "Inserted/Updated 49639 records into PostgreSQL.\n",
      "Batch 37 processed in 167.15 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50532\n",
      "        \n",
      "Inserted/Updated 49695 records into PostgreSQL.\n",
      "Batch 38 processed in 191.79 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50893\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50893\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50893\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50893\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 50893\n",
      "        \n",
      "Inserted/Updated 49666 records into PostgreSQL.\n",
      "Batch 39 processed in 161.84 seconds\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|   id|semester|   code|      description|units|instructor_id|   instructor_name|srcode|        fullname|   campus|             college|             program|major|yearlevel|curriculum| class_section|grade_final|grade_reexam|status|grade_numeric|grade_classification|start_year|       year_sem|program_id|     processing_time|schoolyear|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "|  20250404141549500|20250404141549500...|             38682|             2023-2024|24d8fa0e-8fc8-4f3...|38682|   FIRST|SCI 401|General Chemistry|    4|       -10904|LN-10904, FN-10904| 58100|LN58100, FN58100|ALANGILAN|College of Engine...|BS Geodetic Engin...| null|    FIRST| 2023-2024|BSGEODENG-1101|       2.00|        null|PASSED|         2.00|              NORMAL|      2023|2023-2024-FIRST|        49|2025-04-04 14:15:...| 2023-2024|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----+--------+-------+-----------------+-----+-------------+------------------+------+----------------+---------+--------------------+--------------------+-----+---------+----------+--------------+-----------+------------+------+-------------+--------------------+----------+---------------+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Hudi table exists. Proceeding...\n",
      "Hudi table initialized successfully.\n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "\n",
      "        Stream Status: Processing new data\n",
      "        Data Available: True\n",
      "        Trigger Active: True\n",
      "        Records Processed: 51024\n",
      "        \n",
      "Inserted/Updated 13516 records into PostgreSQL.\n",
      "Batch 40 processed in 155.85 seconds\n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n",
      "\n",
      "        Stream Status: Getting offsets from KafkaV2[Subscribe[source.public.raw_grades]]\n",
      "        Data Available: False\n",
      "        Trigger Active: True\n",
      "        Records Processed: 0\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import psycopg2, os\n",
    "from psycopg2.extras import execute_values\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from scripts.data_cleaners import BaseDataCleaner, AcademicDataCleaner, GradeDataCleaner\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (    \n",
    "    from_json, col, when, lit, current_timestamp, \n",
    "    concat_ws, split, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, \n",
    "    FloatType, TimestampType, DecimalType\n",
    ")\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('etl/scripts')  # Add scripts directory to path\n",
    "from scripts.data_cleaners import AcademicDataCleaner, BaseDataCleaner\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),  # character varying → StringType()\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),  # integer → IntegerType()\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"college\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "hudi_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),  # bigint → LongType()\n",
    "    StructField(\"schoolyear\", StringType(), True),\n",
    "    StructField(\"semester\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),\n",
    "    StructField(\"instructor_id\", StringType(), True),\n",
    "    StructField(\"instructor_name\", StringType(), True),\n",
    "    StructField(\"srcode\", StringType(), True),\n",
    "    StructField(\"fullname\", StringType(), True),\n",
    "    StructField(\"campus\", StringType(), True),\n",
    "    StructField(\"college\", StringType(), True),\n",
    "    StructField(\"program\", StringType(), True),\n",
    "    StructField(\"major\", StringType(), True),\n",
    "    StructField(\"yearlevel\", StringType(), True),\n",
    "    StructField(\"curriculum\", StringType(), True),\n",
    "    StructField(\"class_section\", StringType(), True),\n",
    "    StructField(\"grade_final\", StringType(), True),\n",
    "    StructField(\"grade_reexam\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"grade_numeric\", DecimalType(5,2), True),  # Changed from FloatType to DecimalType\n",
    "    StructField(\"grade_classification\", StringType(), True),\n",
    "    StructField(\"start_year\", IntegerType(), True),  # Changed from StringType to IntegerType\n",
    "    StructField(\"year_sem\", StringType(), True),\n",
    "    StructField(\"program_id\", IntegerType(), True),\n",
    "    StructField(\"processing_time\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define transform function\n",
    "def transform(df, spark):\n",
    "    \"\"\"Cleans and processes the extracted data.\"\"\"\n",
    "    grade_cleaner = GradeDataCleaner()\n",
    "    df = BaseDataCleaner.standardize_case(df, ['grade_final', 'campus', 'semester', 'schoolyear'])\n",
    "    df = AcademicDataCleaner.clean_semesters(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'semester')\n",
    "    \n",
    "    df = BaseDataCleaner.clean_strings(df, [\n",
    "        'schoolyear', 'semester', 'code', 'description', 'units', 'instructor_id', \n",
    "        'instructor_name', 'srcode', 'fullname', 'campus', 'program', \n",
    "        'grade_final', 'grade_reexam', 'status', 'major', 'curriculum', 'class_section'\n",
    "    ])\n",
    "    \n",
    "    df = AcademicDataCleaner.clean_schoolyear(df)\n",
    "    df = grade_cleaner.process_grades(df)\n",
    "    df = BaseDataCleaner.remove_null_strings(df, 'program')\n",
    "    \n",
    "    df = grade_cleaner.allow_numerical_data(df, \"grade_reexam\")\n",
    "\n",
    "    df = AcademicDataCleaner.cast_columns(df, [(\"id\", \"int\"), (\"units\", \"int\"), \n",
    "                                               (\"grade_numeric\", \"decimal(5,2)\")])\n",
    "    \n",
    "    df = grade_cleaner.filter_incomplete_grades(df)\n",
    "\n",
    "    df = AcademicDataCleaner.get_valid_schoolyears(df)\n",
    "\n",
    "    df = AcademicDataCleaner.create_yearsem_order(df)\n",
    "    df = AcademicDataCleaner.map_program_ids(df, spark, \"C:/LEONAIDAS/program_with_id.csv\")\n",
    "\n",
    "    # Add processing_time column\n",
    "    #df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Configure Spark with Kafka and Hudi dependencies\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaToHudiProcessor\")\n",
    "         .master(\"local[*]\")\n",
    "         .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "         .config('spark.jars.packages', \n",
    "                ('org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,'  # Match your Spark version\n",
    "                 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0,'\n",
    "                 'org.postgresql:postgresql:42.7.5'))  # PostgreSQL driver\n",
    "         .config('spark.hadoop.javax.jdo.option.ConnectionDriverName', 'org.postgresql.Driver')\n",
    "         .config(\"spark.sql.extensions\", \"org.apache.hudi.spark3.sql.HoodieSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "         .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "         .config(\"spark.sql.streaming.checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         # Add these network configurations\n",
    "         .config(\"spark.driver.host\", \"localhost\")\n",
    "         .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "         .config(\"spark.network.timeout\", \"600s\")\n",
    "         .config(\"spark.local.dir\", \"C:/tmp/spark-temp\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"C:/tmp/spark_warehouse\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Optimized Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:29092\",\n",
    "    \"subscribe\": \"source.public.raw_grades\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"PLAINTEXT\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"fetchOffset.numRetries\": \"5\",\n",
    "    \"fetch.max.wait.ms\": \"5000\",\n",
    "    \"maxOffsetsPerTrigger\": \"50000\" # Adjust based on your needs\n",
    "}\n",
    "\n",
    "# Read from Kafka with improved configuration\n",
    "kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load())\n",
    "\n",
    "# Parse JSON with error handling\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(\n",
    "                 col(\"value\").cast(\"string\"),\n",
    "                 schema\n",
    "             ).alias(\"data\"))\n",
    "             .select(\"data.*\"))\n",
    "\n",
    "# Enhanced Hudi options for update handling\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'grades',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'schoolyear',\n",
    "    'hoodie.datasource.write.table.name': 'grades',\n",
    "    'hoodie.datasource.write.operation': 'upsert',  # Use upsert for updates\n",
    "    'hoodie.datasource.write.precombine.field': 'processing_time',  # Change back to processing_time\n",
    "    'hoodie.upsert.shuffle.parallelism': '2',\n",
    "    'hoodie.insert.shuffle.parallelism': '2',\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': '2',\n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': '2',\n",
    "    'hoodie.write.concurrency.mode': 'optimistic_concurrency_control',\n",
    "    'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.InProcessLockProvider',\n",
    "}\n",
    "\n",
    "def initialize_hudi_table():\n",
    "    table_path = \"C:/tmp/spark_warehouse/from_kafka\"\n",
    "    \n",
    "    if not os.path.exists(table_path) or not os.listdir(table_path):\n",
    "        print(\"Hudi table does not exist. Initializing now...\")\n",
    "\n",
    "        empty_df = spark.createDataFrame([], hudi_schema)\n",
    "        empty_df.write.format(\"hudi\") \\\n",
    "            .options(**hudi_options) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(table_path)\n",
    "        \n",
    "        print(\"Hudi table initialized successfully.\")\n",
    "    else:\n",
    "        try:\n",
    "            spark.read.format(\"hudi\").load(table_path).show(1)\n",
    "            print(\"Hudi table exists. Proceeding...\")\n",
    "        except AnalysisException:\n",
    "            print(\"Error reading Hudi table. Reinitializing...\")\n",
    "            empty_df = spark.createDataFrame([], hudi_schema)\n",
    "            empty_df.write.format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save(table_path)\n",
    "        \n",
    "        print(\"Hudi table initialized successfully.\")\n",
    "\n",
    "def write_to_postgres(transformed_df):\n",
    "    \"\"\"\n",
    "    Writes the transformed Spark DataFrame to PostgreSQL using Psycopg2.\n",
    "    Uses batch insert/update for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    db_params = {\n",
    "        \"dbname\": \"caist_db_v4\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"postgres\",\n",
    "        \"host\": \"192.168.20.11\",\n",
    "        \"port\": \"5432\"\n",
    "    }\n",
    "    \n",
    "    # Collect data from Spark DataFrame into a list of tuples\n",
    "    data = [tuple(row) for row in transformed_df.collect()]\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to write.\")\n",
    "        return\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO processed_grades_store (\n",
    "            id, schoolyear, semester, code, description, units, instructor_id, \n",
    "            instructor_name, srcode, fullname, campus, college, program, major, yearlevel, \n",
    "            curriculum, class_section, grade_final, grade_reexam, status, \n",
    "            grade_numeric, grade_classification, start_year, year_sem, program_id\n",
    "        ) VALUES %s \n",
    "        ON CONFLICT (id) DO UPDATE SET \n",
    "            schoolyear = EXCLUDED.schoolyear,\n",
    "            semester = EXCLUDED.semester,\n",
    "            code = EXCLUDED.code,\n",
    "            description = EXCLUDED.description,\n",
    "            units = EXCLUDED.units,\n",
    "            instructor_id = EXCLUDED.instructor_id,\n",
    "            instructor_name = EXCLUDED.instructor_name,\n",
    "            srcode = EXCLUDED.srcode,\n",
    "            fullname = EXCLUDED.fullname,\n",
    "            campus = EXCLUDED.campus,\n",
    "            program = EXCLUDED.program,\n",
    "            major = EXCLUDED.major,\n",
    "            yearlevel = EXCLUDED.yearlevel,\n",
    "            curriculum = EXCLUDED.curriculum,\n",
    "            class_section = EXCLUDED.class_section,\n",
    "            grade_final = EXCLUDED.grade_final,\n",
    "            grade_reexam = EXCLUDED.grade_reexam,\n",
    "            status = EXCLUDED.status,\n",
    "            grade_numeric = EXCLUDED.grade_numeric,\n",
    "            grade_classification = EXCLUDED.grade_classification,\n",
    "            start_year = EXCLUDED.start_year,\n",
    "            year_sem = EXCLUDED.year_sem,\n",
    "            program_id = EXCLUDED.program_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(**db_params) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                execute_values(cur, insert_query, data)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted/Updated {len(data)} records into PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to PostgreSQL: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        if batch_df.rdd.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "            return\n",
    "        \n",
    "        batch_df.cache()\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Ensure the Hudi table is initialized before writing\n",
    "        initialize_hudi_table()\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_df = transform(batch_df, spark)\n",
    "        transformed_df.cache()\n",
    "        \n",
    "        try:\n",
    "            # Write to Hudi\n",
    "            hudi_df = transformed_df.withColumn(\"processing_time\", current_timestamp())\n",
    "            hudi_df.write \\\n",
    "                .format(\"hudi\") \\\n",
    "                .options(**hudi_options) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "            \n",
    "            # Write to PostgreSQL using Psycopg2\n",
    "            write_to_postgres(transformed_df)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            print(f\"Batch {batch_id} processed in {duration:.2f} seconds\")\n",
    "        \n",
    "        finally:\n",
    "            transformed_df.unpersist()\n",
    "            hudi_df.unpersist()\n",
    "            batch_df.unpersist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Start streaming with monitoring\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .outputMode(\"update\")\n",
    "         .option(\"checkpointLocation\", \"C:/tmp/spark_warehouse/checkpoints\")\n",
    "         .trigger(processingTime=\"0 seconds\")\n",
    "         .start())\n",
    "\n",
    "# Enhanced monitoring\n",
    "while query.isActive:\n",
    "    try:\n",
    "        stats = query.status\n",
    "        progress = query.recentProgress\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Stream Status: {stats['message']}\n",
    "        Data Available: {stats.get('isDataAvailable', False)}\n",
    "        Trigger Active: {stats.get('isTriggerActive', False)}\n",
    "        Records Processed: {progress[-1]['numInputRows'] if progress else 0}\n",
    "        \"\"\")\n",
    "        \n",
    "        query.awaitTermination(30)  # Check every 2 minutes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK CREATED HUDI TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Hudi table\n",
    "hudi_df = spark.read \\\n",
    "    .format(\"hudi\") \\\n",
    "    .load(\"C:/tmp/spark_warehouse/from_kafka\")\n",
    "\n",
    "# Show schema\n",
    "print(\"Table Schema:\")\n",
    "hudi_df.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample Data:\")\n",
    "hudi_df.show(5)\n",
    "\n",
    "# Get total count\n",
    "print(f\"\\nTotal Records: {hudi_df.count()}\")\n",
    "\n",
    "# Get statistics by partition (schoolyear)\n",
    "print(\"\\nRecords by Schoolyear:\")\n",
    "hudi_df.groupBy(\"schoolyear\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
